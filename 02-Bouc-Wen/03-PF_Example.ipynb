{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Imports ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import time as timeit\n",
    "import datetime\n",
    "import sys,os\n",
    "\n",
    "## Particle Filter Imports ##\n",
    "import particles\n",
    "from particles import state_space_models as ssm\n",
    "from particles import smc_samplers as ssp\n",
    "from particles import distributions as dists\n",
    "import warnings; warnings.simplefilter('ignore')  # hide warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter Example\n",
    "In this file, I develop an sequential monte carlo squared (SMC2) solution to the identification of a single-degree-of-freedom oscillator (SDOF), in which the nonlinear switch state is turned ON, essentially developing a Bouc-Wen SDOF system. The nonlinear stiffness contribution is turned on for this part of the example, as part of the switch state for this system. \n",
    "\n",
    "SMC2, which falls into the particle filter class of Bayesian filtering approximations, incorporates a small set of hyperparameters used to adjust the sampling properties of the embedded Monte Carlo samplers used in this technique. For this example, these parameters were primarily selected according to the suggestions by Chopin and Papaspiliopoulos in their book \"An Introduction to Sequential Monte Carlo\"<sup>1</sup>. Specific values associated with MC sample points were selected to ensure sufficient convergence on the particular case study. These variables include: \n",
    " \n",
    "1. $N_{\\theta}$, stated as ```init_Nx``` in the code. This variable represents the number of particles used in the SMC layer on the parameters. \n",
    "2. $AR$, states as ```ar_to_increase_Nx``` in the code. This variable represents the acceptance rate of the PMMH kernals used in the SMC layer of the filter. It is typically set to a value around 0.1, such that when the acceptance rate of the PMMH kernels falls below 10%, the number of particles on the parameters, $N_{\\theta}$, is doubled. \n",
    "3. $N_x$, stated as ```N``` in the code. This variable represents the number of particle filter samples on the states, where a particle filter is attached to each of the $N_{\\theta}$ particles approximating the parameters in order to form an unbiased estimate of the intractable marginal likelihood, $p(y_k|\\theta)$.\n",
    "4. The resampling scheme, stated as ```resampling``` in the code. As noted in <sup>1</sup>, several valid options are available for the resampling scheme. In this case, we have chosen to use a 'systematic' resampling scheme.<sup>2</sup>\n",
    "5. $ESS$, stated as ```ESSrmin``` in the code. This variable represents the effective sample size threshold. If the effective sampling size falls below this threshold, the $N_x$ particles approximating the states are resampled and then moved according to a particle Markov chain Monte Carlo (PMCMC) kernel that leaves the current target distribution invariant. This value is usually set to $N/2$. \n",
    "\n",
    "This example runs 50 inference trials using varied prior information on the parameters, simulating different assertions an experimentalist might make in a practical identification scenario. Outputs from this model include:\n",
    "1. The mean and standard deviation of the state and log(parameters) over the inference period. \n",
    "2. The mean of the state and parameters over the inference period. \n",
    "3. The mode of the state and parameters over the inference period. \n",
    "4. The computational model response built from the inferred parameters with respect to the input signal used for inference.\n",
    "5. The runtime for each inference trial. \n",
    "\n",
    "In running this example, I found that the majority of the candidate priors developed convergent posterior models given ```init_Nx = 10``` and ```N = 50``` particles at the initialization of the filter.\n",
    "\n",
    "For some cases, including sample priors $[8,10,25,31,32,39,42,47]$, these initialization particles were not sufficient, and the number of samples needed to be increased at significantly higher computational cost. As such, this candidate prior was evaluated with ```init_Nx = 15``` and ```N = 75```, which was found to generate convergent behavior in the posterior model. \n",
    "\n",
    "Additional hyperparameters in an experimental scenario include the process noise covariance $Q$ and the process noise covariance $R$. However, these terms are assumed known for this problem. \n",
    "\n",
    "The SMC$^2$ implementation expressed herein is drawn from the python library particles<sup>3</sup>. Some small modifications were made to adapt the library to this problem, and are shown in detail in the code below. \n",
    "\n",
    "__Developed by__: Alana Lund (Purdue University) \\\n",
    "__Last Updated__: 22 Sept. 2021 \\\n",
    "__License__: AGPL-3.0\n",
    "\n",
    "### References\n",
    "<sup>1</sup> N. Chopin and O. Papaspiliopoulos. An Introduction to Sequential Monte Carlo. _Springer Series in Statistics_ (2020). \n",
    "\n",
    "<sup>2</sup> J. Hol, T. Schon, and F. Gustafsson. On Resampling Algorithms for Particle Filters. _Nonlinear Statistical Signal Processing Workshop_ (2006). \n",
    "\n",
    "<sup>3</sup> N. Chopin, particles v0.1, (2021).[https://github.com/nchopin/particles](https://github.com/nchopin/particles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assert File Names\n",
    "inFile = '../04-Data/Bouc-Wen/inferenceInput'\n",
    "\n",
    "infData = np.load(inFile + '.npz')\n",
    "\n",
    "dt = infData['dt']                 # time step [sec]\n",
    "time = infData['time']             # time history [sec]\n",
    "inpAcc = infData['inpAcc']         # observations on input acceleration [m/sec^2]\n",
    "states = infData['statesPNoise']   # states (for post-inference validation) [m,m/sec,m]\n",
    "respAcc = infData['accPMNoise']    # observations on response acceleration [m/sec^2]\n",
    "Q = infData['Qfactor']             # process noise contributions, independent std. dev. per state [m,m/sec,m]\n",
    "R = infData['Rfactor']             # measurement noise contribution [m/sec^2]\n",
    "m = infData['m']                   # mass [kg]\n",
    "ics = infData['ics']               # true initial conditions of the system [m, m/sec, m]\n",
    "par = infData['par']               # true parameters of the system [xi (-), wn (rad/sec), beta [1/m^2], n [-], gamma [1/m^2]] \n",
    "\n",
    "### Lay Out Problem Dimensionality ###\n",
    "nInf = 8                     # Number of inferred variables [-]\n",
    "nState = states.shape[0]     # Number of states [-]\n",
    "nPar = nInf - nState         # Number of parameters [-]\n",
    "samps = len(time)            # Number of system measurements [-]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BW Model Class and Generate an Instance of the Class\n",
    "Here's where I figure out how to turn the general example in this tutorial into a use case for the review paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoucWen(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    The default parameters initialized below tell the model that we anticipate accepting\n",
    "    parameters, but that they may be constants (particle filtering on the states) \n",
    "    or distributions (SMC^2 on the states and parameters). Those parameters which are\n",
    "    given prior distributions and passed to the algorithm will be inferred, and the \n",
    "    remainder of these defaults will be treated as constants. \n",
    "    \n",
    "    To accommodate not being able to modify the constants after I initialize the SMC^2 \n",
    "    algorithm, I directly load the excitation into the initialization statement, so that\n",
    "    all portions of the time history are accessible. This give a little more computational \n",
    "    overhead, but it should not be excessive, as we are putting this call in the class \n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    infData = np.load('../04-Data/Bouc-Wen/inferenceInput.npz')\n",
    "    \n",
    "    default_params = {'exc': infData['inpAcc'],\n",
    "                      'dt': infData['dt'], 'Q':infData['Qfactor'], 'R':infData['Rfactor'],\n",
    "                      'xi':np.log(0.05), 'wn':np.log(3.), \n",
    "                      'beta':np.log(2.), 'n':np.log(2.), 'gamma':np.log(1.)}\n",
    "    # I've organized the default params dictionary such that the rows contain\n",
    "    # (1) Input excitation\n",
    "    # (2) Constant Parameters, including the time step, and noise terms\n",
    "    # (3) Linear parameters to be inferred\n",
    "    # (4) Nonlinear parameters to be inferred\n",
    "    \n",
    "    def PX0(self):\n",
    "        \"\"\"\n",
    "        Defines the prior distribution on the states. \n",
    "        \"\"\"\n",
    "        mu0 = np.zeros(3)\n",
    "        sig0 = np.square(np.array([0.05, 0.05, 0.05]))\n",
    "        return dists.MvNormal(loc=mu0, scale=sig0, cov=np.eye(3))\n",
    "    \n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"\n",
    "        Defines the state transition function. \n",
    "        \n",
    "        t = time index [-]\n",
    "        xp = previous state estimate\n",
    "        \"\"\"\n",
    "        ## Remove Transformation on the Parameters ##\n",
    "        par = np.array([np.exp(self.xi), np.exp(self.wn), \n",
    "                        np.exp(self.beta), np.exp(self.n), np.exp(self.gamma)])\n",
    "\n",
    "        ## Define State Transition ##\n",
    "        x1dot = xp[0,0] + self.dt*xp[0,1]\n",
    "        x2dot = xp[0,1] + self.dt*(-self.exc[t] - (2*par[0]*par[1])*xp[0,1] \n",
    "                            - np.square(par[1])*xp[0,2])\n",
    "        x3dot = xp[0,2] + self.dt*(xp[0,1] - par[2]*np.absolute(xp[0,1])*\n",
    "                    np.power(np.absolute(xp[0,2]), par[3]-1)*xp[0,2] \n",
    "                    - par[4]*xp[0,1]*np.power(np.absolute(xp[0,2]), par[3]))\n",
    "        \n",
    "        ## Format Results ##\n",
    "        muT = np.stack((x1dot, x2dot, x3dot), axis=0)\n",
    "        sigT = np.square(self.Q)\n",
    "        return dists.MvNormal(loc=muT, scale=sigT, cov=np.eye(3))\n",
    "    \n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"\n",
    "        Defines the observation function.\n",
    "        \n",
    "        t = time index [-]\n",
    "        xp = previous state estimate\n",
    "        x = current state estimate\n",
    "        \"\"\"\n",
    "        ## Remove Transformation on the Parameters ##\n",
    "        par = np.array([np.exp(self.xi), np.exp(self.wn)])\n",
    "         \n",
    "        ## Define Obervation Function ##\n",
    "        muR = -(2*par[0]*par[1])*x[:,1] - np.square(par[1])*x[:,2]\n",
    "        sigR = np.square(self.R)\n",
    "        return dists.Normal(loc=muR, scale=sigR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(x, dt, exc=None):\n",
    "    \"\"\"\n",
    "    State transition model for a SDOF oscillator with a Bouc-Wen switch \n",
    "    state, given that alpha = 0, and therefore the Bouc-Wen component is \n",
    "    switched off.\n",
    "      \n",
    "    x = 1x8 vector of states (disp [m], vel [m/sec], Bouc-Wen disp [m]) \n",
    "                and parameters to be inferred (log(xi),log(wn), log(beta), \n",
    "                log(n), log(gamma)). \n",
    "    dt = sampling rate [sec]\n",
    "    exc = input excitation at current time step [m/sec^2]\n",
    "    \"\"\"\n",
    "    if exc is None:\n",
    "        exc = np.zeros(x[1].shape)\n",
    "      \n",
    "    par = np.exp(x[3:]) \n",
    "            \n",
    "    x1dot = x[0] + dt*x[1]\n",
    "    x2dot = x[1] + dt*(-exc - (2*par[0]*par[1])*x[1] - np.square(par[1])*x[2])\n",
    "    x3dot = x[2] + dt*(x[1] - par[2]*np.absolute(x[1])*\n",
    "                np.power(np.absolute(x[2]), par[3]-1)*x[2] \n",
    "                - par[4]*x[1]*np.power(np.absolute(x[2]), par[3]))\n",
    "\n",
    "    return np.concatenate((np.stack((x1dot, x2dot, x3dot), axis=0)\n",
    "                           , x[3:]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SMC$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PF Specific Parameters ###\n",
    "# Changing the number of particles here will generate \n",
    "# separate save files. These can be combined into a full\n",
    "# example case on all 50 prior distributions using the \n",
    "# 03.1-Compile_PF_Runs code. \n",
    "N_theta = 10          # Number of particles on the parameters\n",
    "acceptRate = 0.1      # acceptance rate of the PMMH kernel\n",
    "N_x = 50              # Number of particles on the states\n",
    "resamp = 'systematic' # resampling type\n",
    "ESS_thresh = 0.5      # effective sample size threshold\n",
    "\n",
    "seed = 42568                            # Random seed to initialize stochastic sampling\n",
    "checkStep = len(inpAcc)/5               # Meter the progress through the data              \n",
    "outFile = ('../04-Data/Linear/outputPF_Nx' + \n",
    "           str(N_x) + '_Ntheta' + str(N_theta)) # output file\n",
    "\n",
    "### Load Prior Distributions on the Parameters ###\n",
    "parPriors = np.loadtxt('../04-Data/parameter_priors.txt')\n",
    "parMeans = parPriors[:,::2]\n",
    "parStds = parPriors[:,1::2]\n",
    "\n",
    "### Generate Storage Over Inferred States/Parameters ###\n",
    "muHist = np.zeros((parPriors.shape[0],nInf, samps))\n",
    "    # mean of the inferred parameters for each inference trial\n",
    "    # over the observation period. This is what the PF directly\n",
    "    # outputs\n",
    "stdHist = np.zeros((parPriors.shape[0],nInf, samps))\n",
    "stdHist[:,:3,0] = 0.05*np.ones((parPriors.shape[0], nState))\n",
    "    # standard deviation of the inferred parameters for each \n",
    "    # inference trial over the observation period. This is \n",
    "    # what the PF directly outputs\n",
    "meanHist = np.zeros((parPriors.shape[0],nInf, samps))\n",
    "    # mean of the inferred states and the underlying parameters\n",
    "    # for each inference trial over the observation period. This\n",
    "    # measure transforms the parameters to a lognormal distribution\n",
    "    # such that the statistics on the true parameter values can be\n",
    "    # extracted. \n",
    "modeHist = np.zeros((parPriors.shape[0],nInf, samps))\n",
    "    # mode of the inferred states and the underlying parameters\n",
    "    # for each inference trial over the observation period. This\n",
    "    # measure transforms the parameters to a lognormal distribution\n",
    "    # such that the statistics on the true parameter values can be\n",
    "    # extracted. \n",
    "modStates = np.zeros((parPriors.shape[0],nInf, samps))\n",
    "    # Response history of the inferred system given the input\n",
    "    # excitation. Essentially, we're remodeling the behavior of \n",
    "    # the system given our selections on point estimates of the \n",
    "    # parameters from the posterior. \n",
    "runTime = np.zeros((parPriors.shape[0]))\n",
    "    # Computational time for each inference trial. \n",
    "Nx = np.zeros((parPriors.shape[0],samps))\n",
    "    # Tracks the increase in number of particles on the parameters\n",
    "    # over the inference history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For Each Inference Trial... ##\n",
    "for j in range(parPriors.shape[0]):\n",
    "    ### Set a Unique Random Seed ###\n",
    "    np.random.seed(seed+j*7)\n",
    "    \n",
    "    ### Structure the Prior Distributions on the Parameters ###\n",
    "    prior_dict = {'xi': dists.Normal(loc=parMeans[j,0], scale=parStds[j,0]), \n",
    "                  'wn': dists.Normal(loc=parMeans[j,1], scale=parStds[j,1]), \n",
    "                  'beta': dists.Normal(loc=parMeans[j,2], scale=parStds[j,2]), \n",
    "                  'n': dists.Normal(loc=parMeans[j,3], scale=parStds[j,3]), \n",
    "                  'gamma': dists.Normal(loc=parMeans[j,4], scale=parStds[j,4])}\n",
    "    my_prior = dists.StructDist(prior_dict)\n",
    "    \n",
    "    ### Initialize Filter Layers ###\n",
    "    bwMod_smc2 = ssp.SMC2(ssm_cls=BoucWen, data=respAcc, prior=my_prior,init_Nx=N_theta, \n",
    "                       ar_to_increase_Nx=acceptRate)\n",
    "                        # By default, we are asserting that the importance distributions\n",
    "                        # be formed according to the 'Bootstrap' methodology, i.e. that\n",
    "                        # the transition density forms the importance distribution\n",
    "    bwSMC2 = particles.SMC(fk=bwMod_smc2, N=N_x, resampling = resamp,\n",
    "                        ESSrmin = ESS_thresh, moments=True)\n",
    "\n",
    "    ### Run Filter ###\n",
    "    print('Iteration %d:'%(j))\n",
    "    \n",
    "    ## Time Each Inference Trial ##\n",
    "    t0 = timeit.time()\n",
    "    tf = t0\n",
    "    \n",
    "    for i in range(len(inpAcc)):\n",
    "        ##  Check Filter Convergence ##\n",
    "        # For this problem, run time for a single case is < 8 hours \n",
    "        if ((tf-t0)/3600 > 8):\n",
    "            break\n",
    "            \n",
    "        ## Step the SMC^2 Filter Forward ##\n",
    "        next(bwSMC2)\n",
    "        \n",
    "        ## Record Marginal Posterior on the States given Y_t, t<T ##\n",
    "        muHist[j,:3,i] = np.average(np.array([np.average(pf.X, weights=pf.W, axis=0) for pf in bwSMC2.X.pfs]), \n",
    "                                 weights=bwSMC2.W, axis=0) \n",
    "        stdHist[j,:3,i] = np.sqrt(np.average(np.array([np.average(np.square(pf.X), \n",
    "                                                        weights=pf.W, axis=0) for pf in bwSMC2.X.pfs]), \n",
    "                                 weights=bwSMC2.W, axis=0) - np.square(muHist[j,:3,i])) \n",
    "        meanHist[j,:3,i] = muHist[j,:3,i]\n",
    "        modeHist[j,:3,i] = muHist[j,:3,i]\n",
    "        \n",
    "        ## Report Progress through the Time History ##\n",
    "        if (i+1)%checkStep == 0:\n",
    "            now = datetime.datetime.now()\n",
    "            print('\\tI am {} seconds in at time {}'.format((i+1)*dt, now))\n",
    "            print('\\t\\tNx = %d'%(bwSMC2.X.Nxs[i]))\n",
    "\n",
    "        tf = timeit.time()\n",
    "   \n",
    "    ## If SMC^2 Fails to Converge ##\n",
    "    if muHist[j,3,-1] == 0:\n",
    "        print('\\tTrial Exceeded Alloted Computation Time.')\n",
    "        ## Store Results ##\n",
    "        for i in range(1,samps):\n",
    "            muHist[j,:,i] = np.array([0,0,0, -0.1054, 2.3, 3.4, 1.9, 3.4])\n",
    "            stdHist[j,:,i] = 0.01*np.ones(8)\n",
    "            meanHist[j,:,i] = np.concatenate((muHist[j,:nState,i], \n",
    "                                    np.exp(muHist[j,nState:,i] + np.square(stdHist[j,nState:,i])/2)))\n",
    "            modeHist[j,:,i] = np.concatenate((muHist[j,:nState,i], np.exp(muHist[j,nState:,i] \n",
    "                                    - np.square(stdHist[j,nState:,i]))))\n",
    "      \n",
    "    ## If SMC^2 Converges ##\n",
    "    else:\n",
    "        ## Adapt Parameter Results to Array Format ## \n",
    "        pfMeans = np.array([m['mean'] for m in bwSMC2.summaries.moments])\n",
    "        pfVars = np.array([m['var'] for m in bwSMC2.summaries.moments])\n",
    "\n",
    "        for i in range(len(pfMeans)):\n",
    "            muHist[j,3:,i] = np.array([pfMeans[i][4], pfMeans[i][3], pfMeans[i][0], pfMeans[i][2], pfMeans[i][1]])\n",
    "            stdHist[j,3:,i] = np.sqrt(np.array([pfVars[i][4], pfVars[i][3], pfVars[i][0], pfVars[i][2], pfVars[i][1]]))\n",
    "            meanHist[j,3:,i] = np.exp(muHist[j,3:,i] + np.square(stdHist[j,3:,i])/2)\n",
    "            modeHist[j,3:,i] = np.exp(muHist[j,3:,i] - np.square(stdHist[j,3:,i]))\n",
    "    \n",
    "    ### Rerun Model with Point Estimates (Mode) of Inferred Parameters ###\n",
    "    modStates[j,:,0] = np.concatenate((np.zeros((3,)), np.log(modeHist[j,3:,-1])))\n",
    "    for i in range(1,len(time)):\n",
    "        modStates[j,:,i] = fx(modStates[j,:,i-1], dt, exc=inpAcc[i-1])   \n",
    "\n",
    "    ## Store Progression of Theta-Particles and RunTime ##\n",
    "    Nx[j,:] = sdofSMC2.X.Nxs\n",
    "    runTime[j] = (tf-t0)/60\n",
    "    \n",
    "    ## Print Results Summary and Save ##\n",
    "    print('\\tComputation Time = %d minutes and %d seconds' %(np.floor((tf-t0)/60), \n",
    "                                                        np.floor((tf-t0) - 60*np.floor((tf-t0)/60))) )\n",
    "    print('\\tMode of Final Parameter Distributions: \\n\\t\\txi = %.4f,\\n\\t\\twn = %.4f,\\n\\t\\tbeta = %.4f,\\n\\t\\tn = %.4f,\\n\\t\\tgamma = %.4f\\n'\n",
    "          %(modeHist[j,3,-1],modeHist[j,4,-1],modeHist[j,5,-1],modeHist[j,6,-1],modeHist[j,7,-1]))\n",
    "\n",
    "    ## Save Outputs After Each Inference Trial ##\n",
    "    np.savez(outFile, muHist = muHist,stdHist=stdHist, meanHist=meanHist, \n",
    "             modeHist=modeHist, modStates = modStates, Nxs = Nx, runTime = runTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Capacity of the Inferred Models\n",
    "The goal of this section is to develop a prediction of the response behavior of the system to a secondary event, given the models which have been inferred from the primary excitation. \n",
    "\n",
    "### Load Inference Data\n",
    "This becomes an optional start point in the code. If the data for the PF has already been generated, it can simply be loaded in for the predictive analysis instead of rerunning the previous block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = '../04-Data/Bouc-Wen/outputPF'  # This output file is generated by combining all output\n",
    "                                        # cases on N_theta and N_x. \n",
    "outData = np.load(outFile + '.npz')\n",
    "\n",
    "muHist = outData['muHist']         # inference history of untransformed state/par means\n",
    "stdHist = outData['stdHist']       # inference history of state/par standard deviations\n",
    "meanHist = outData['meanHist']     # inference history of transformed state/par means\n",
    "modeHist = outData['modeHist']     # inference history of transformed state/par modes\n",
    "modStates = outData['modStates']  # states that have been remodeled based on the final modes of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Secondary Input Excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predInFile = '../04-Data/Bouc-Wen/predInp_BLWN'\n",
    "predOutFile = '../04-Data/Bouc-Wen/predOutPF'\n",
    "\n",
    "infData = np.load(predInFile + '.npz')\n",
    "\n",
    "dt = infData['dt']                            # time step [sec]\n",
    "time = infData['time']                        # time history [sec]\n",
    "predBase = infData['predInp']                 # observations on input acceleration [m/sec^2]\n",
    "predStatesTrue = infData['predStatesPNoise']  # states (for post-prediction validation) [m,m/sec]\n",
    "predRespTrue = infData['predAccPMNoise']      # observations on response acceleration [m/sec^2]\n",
    "Q = infData['Qfactor']                        # process noise contributions, independent std. dev. per state [m,m/sec]\n",
    "R = infData['Rfactor']                        # measurement noise contribution [m/sec^2]\n",
    "m = infData['m']                              # mass [kg]\n",
    "ics = infData['ics']                          # true initial conditions of the system [m, m/sec]\n",
    "par = infData['par']                          # true parameters of the system [xi (-), wn (rad/sec)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictive Distribution on the States over Secondary Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive Distribution from Inferred Results\n",
      "Case 0\n",
      "Case 1\n",
      "Case 2\n",
      "Case 3\n",
      "Case 4\n",
      "Case 5\n",
      "Case 6\n",
      "Case 7\n",
      "Case 8\n",
      "Case 9\n",
      "Case 10\n",
      "Case 11\n",
      "Case 12\n",
      "Case 13\n",
      "Case 14\n",
      "Case 15\n",
      "Case 16\n",
      "Case 17\n",
      "Case 18\n",
      "Case 19\n",
      "Case 20\n",
      "Case 21\n",
      "Case 22\n",
      "Case 23\n",
      "Case 24\n",
      "Case 25\n",
      "Case 26\n",
      "Case 27\n",
      "Case 28\n",
      "Case 29\n",
      "Case 30\n",
      "Case 31\n",
      "Case 32\n",
      "Case 33\n",
      "Case 34\n",
      "Case 35\n",
      "Case 36\n",
      "Case 37\n",
      "Case 38\n",
      "Case 39\n",
      "Case 40\n",
      "Case 41\n",
      "Case 42\n",
      "Case 43\n",
      "Case 44\n",
      "Case 45\n",
      "Case 46\n",
      "Case 47\n",
      "Case 48\n",
      "Case 49\n",
      "\n",
      "Indices of Unstable Predictive Distributions:\n",
      "4\n",
      "20\n",
      "28\n",
      "30\n",
      "38\n",
      "46\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "### Set Constants for Predictive Sampling ###\n",
    "nPriors = muHist.shape[0]    # Number of inference trials [-]\n",
    "nSamps = 500                 # Number of samples on the inference posterior [-]\n",
    "seeds = [8192,3245]          # seeds for random number generator\n",
    "\n",
    "### Generate Storage Over Predicted States ###\n",
    "totalSamps = np.zeros((nSamps*nPriors, nState, len(time)))\n",
    "    # Predicted states based on simulations results for all posterior\n",
    "    # samples from all inference trials\n",
    "meanPred = np.zeros((nPriors, nState, len(time)))\n",
    "    # Mean of the predicted states for each inference trial.\n",
    "stdPred = np.zeros((nPriors,nState, len(time)))\n",
    "    # Standard deviation of the predicted states for each inference trial. \n",
    "\n",
    "### Run Predictive Trials on All Candidate Models ###\n",
    "print('Predictive Distribution from Inferred Results')\n",
    "for j in range(nPriors):\n",
    "    print('Case %d'%(j))\n",
    "    ## Random Samples on the States and Parameters, based on Inferred Posterior ##\n",
    "    np.random.seed(seeds[0]+j)\n",
    "    rSamp = np.random.multivariate_normal(np.zeros(nInf), np.eye(nInf), nSamps)\n",
    "    predSamps = muHist[j,:,-1] + stdHist[j,:,-1]*rSamp\n",
    "\n",
    "    ## Random Samples on the Transition Noise ##\n",
    "    np.random.seed(seeds[1]+j)\n",
    "    noise = Q.reshape(-1,1)*np.random.multivariate_normal(np.zeros(nState), np.eye(nState), \n",
    "                                                          (nSamps, len(time))).transpose((0, 2, 1))\n",
    "\n",
    "    ## Prepare Response Storage ##\n",
    "    predStates = np.zeros((nSamps, nState,len(time)))\n",
    "    predStates[:,:,0] = predSamps[:,:nState]\n",
    "\n",
    "    for i in range(nSamps):\n",
    "        for tt in range(1,len(time)):\n",
    "            predStates[i,:,tt] = fx(np.concatenate((predStates[i,:, tt-1], predSamps[i,nState:])), \n",
    "                                                 dt, exc = predBase[tt-1])[:nState] + noise[i,:,tt-1]\n",
    "    \n",
    "    ## Store Results from Predictive Sample Runs ##\n",
    "    meanPred[j,:,:] = np.mean(predStates, axis = 0)\n",
    "    stdPred[j,:,:] = np.sqrt(np.mean(np.square(predStates), axis=0) - np.square(meanPred[j,:,:])) \n",
    "    totalSamps[j*nSamps:(j+1)*nSamps,:,:] = predStates\n",
    "    \n",
    "### Remove Unstable Results from the Overall Assessment ###\n",
    "# Candidate models can become unstable during inference (due to \n",
    "# computational issues such as singularities in the covariance \n",
    "# matrices) or manifest instability during predictive modeling\n",
    "# due to combinations of the selected parameters which result in\n",
    "# model divergence. Here we extract these cases so that they don't \n",
    "# interfere with the statistics of the main results. \n",
    "stabilityInd = np.ones(nPriors)\n",
    "totalStabilityInd = np.ones(nPriors*nSamps)\n",
    "\n",
    "print('\\nIndices of Unstable Predictive Distributions:')\n",
    "for i in range(nPriors):\n",
    "    if (np.isnan(meanPred[i,0,-1])) or (np.absolute(meanPred[i,0,-1])>100) or (muHist[i,0,-1] == 0):\n",
    "        stabilityInd[i] = 0 \n",
    "        totalStabilityInd[i*nSamps:(i+1)*nSamps] = np.zeros(nSamps)\n",
    "        print(i)\n",
    "\n",
    "stableMeans = meanPred[stabilityInd != 0,:,:]\n",
    "stableStds = stdPred[stabilityInd != 0,:,:]\n",
    "stableSamps = totalSamps[totalStabilityInd != 0,:,:]\n",
    "\n",
    "### Statistics on all Stable Cases ###\n",
    "meanAll = np.mean(stableSamps, axis = 0)\n",
    "stdAll = np.sqrt(np.mean(np.square(stableSamps), axis=0) - np.square(meanAll)) \n",
    "\n",
    "### Save Output ###\n",
    "np.savez(predOutFile, meanPred = meanPred,stdPred=stdPred, \n",
    "         stableMeans=stableMeans, stableStds=stableStds, meanAll=meanAll, stdAll=stdAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
