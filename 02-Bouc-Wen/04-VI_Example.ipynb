{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Imports ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as writer\n",
    "\n",
    "import time as timeit\n",
    "import datetime\n",
    "import sys,os\n",
    "\n",
    "## Torch Functions ##\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "torch.set_default_dtype(torch.float64) # Set default type to float64 (instead of float32)\n",
    "\n",
    "## Update External Codes ##\n",
    "import importlib\n",
    "\n",
    "## Import Handmade Functions ##\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../00-GeneralFunctions'))\n",
    "import varFam\n",
    "import stochMod_annealing as stochMod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference Example\n",
    "In this file, I develop an automatic differention variational inference (ADVI) solution to the identification of a single-degree-of-freedom oscillator (SDOF), in which the nonlinear switch state is turned ON, essentially developing a Bouc-Wen SDOF system. The nonlinear stiffness contribution is turned on for this part of the example, as part of the switch state for this system. \n",
    "\n",
    "ADVI, which falls into the optimization class of Bayesian filtering approximations, incorporates a small set of hyperparameters used to adjust the properties of the stochastic gradient descent algorithm that optimizes the approximate posterior against the Kullback-Leibler (KL) divergence. For this example, these hyperparameters were selected to develop a reasonable and consistent approximation of the posterior with relatively low variance in the optimization history (evidence lower bound values at each optimization step). These variables include: \n",
    " \n",
    "1. $N_{epoch}$, stated as ```numEpoch``` in the code. This variable represents the number of epochs used in the optimization process. At the beginning of each epoch, the gradients are cleared and new starting values are assigned to the optimization parameters and the learning rates. \n",
    "2. $N_{iter}$, stated as ```epochIter``` in the code. This variable represents the number of optimization iterations used in each epoch. \n",
    "3. $l_r$, stated as ```lr``` in the code. This variable represents the step size amplifier for the optimization step scheduler. The meaning of this can be different depending on which stochastic optimization algorithm you select. For the purposes of this example, we will be using the Adam algorithm. \n",
    "4. $N_{MC}$, stated as ```MCSamp``` in the code. This variable represents the number of MC samples on the inferred states/parameters that will be used as part of reducing to reduce the variance of the stochastic gradient descent algorithm. \n",
    "\n",
    "In addition to these variables, the algorithm used herein also incorporates the option to develop the approximate marginal posteriors through KL-annealing <sup>1</sup>. This process more heavily weights the fit of the model to the data at the beginning of the optimization process. As the optimization progresses, the contribution of the prior to the loss function is more heavily weighted, until the weighting on the prior and on the likelihood is equal at the final optimization iteration. Though the annealing schedule can be generated in many ways, here we select a linear schedule. \n",
    "\n",
    "This example runs 50 inference trials using varied prior information on the parameters, simulating different assertions an experimentalist might make in a practical identification scenario. Outputs from this model include:\n",
    "1. The mean and standard deviation of the log(parameters) over the optimization history.\n",
    "2. The mean and standard deviation of the states at the final step optimization\n",
    "3. The mean and mode of the parameters over the optimization history. \n",
    "4. The computational model response built from the inferred parameters with respect to the input signal used for inference.\n",
    "5. The runtime for each inference trial. \n",
    "\n",
    "The user should note that this algorithm, unlike the others demonstrated in this illustrative example, operates in a 'batch' mode, evaluating all of the data at once, rather than a 'filter' mode, which evaluates data points successively. Though research is ongoing in the development of practical variational filtering approaches, and therefore there is no \"standard\" method that can be extrapolated on for this example, the authors still felt that it was important to give a comparison between the base variational inference approach and the other algorithms. \n",
    "\n",
    "The VI implementation expressed herein is drawn from the python library pyTorch<sup>2</sup> and relies heavily on the work of Kucukelbir et al.<sup>3</sup>. \n",
    "\n",
    "__Developed by__: Alana Lund (Purdue University) \\\n",
    "__Last Updated__: 25 Oct. 2021 \\\n",
    "__License__: AGPL-3.0\n",
    "\n",
    "### References\n",
    "<sup>1</sup> C.-W. Huang, S. Tan, A. Lacoste, A. Courville. Improving the Explorability in Variational Inference with Annealed Variational Objectives. _arXiv preprint arXiv:1809.01818_ (2018).\n",
    "\n",
    "<sup>2</sup> A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, [PyTorch: an imperative style, high-performance deep learning library](https://pytorch.org/) in: Adv. Neural Inf. Process. Syst., 2019: pp. 8024–8035. http://arxiv.org/abs/1912.01703.\n",
    "\n",
    "\n",
    "<sup>3</sup> A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, D. Blei. Automatic Differentiation Variational Inference. _Journal of Machine Learning Research_ (2017). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assert File Names\n",
    "inFile = '../04-Data/Bouc-Wen/inferenceInput'\n",
    "outFile = '../04-Data/Bouc-Wen/outputVI'\n",
    "\n",
    "infData = np.load(inFile + '.npz')\n",
    "\n",
    "dt = infData['dt']                 # time step [sec]\n",
    "time = infData['time']             # time history [sec]\n",
    "inpAcc = infData['inpAcc']         # observations on input acceleration [m/sec^2]\n",
    "states = infData['statesPNoise']   # states (for post-inference validation) [m,m/sec,m]\n",
    "respAcc = infData['accPMNoise']    # observations on response acceleration [m/sec^2]\n",
    "Q = infData['Qfactor']             # process noise contributions, independent std. dev. per state [m,m/sec,m]\n",
    "R = infData['Rfactor']             # measurement noise contribution [m/sec^2]\n",
    "m = infData['m']                   # mass [kg]\n",
    "ics = infData['ics']               # true initial conditions of the system [m, m/sec, m]\n",
    "par = infData['par']               # true parameters of the system [xi (-), wn (rad/sec), beta [1/m^2], n [-], gamma [1/m^2]] \n",
    "\n",
    "### Lay Out Problem Dimensionality ###\n",
    "numInf = 8                     # Number of inferred variables [-]\n",
    "nState = states.shape[0]     # Number of states [-]\n",
    "nPar = numInf - nState         # Number of parameters [-]\n",
    "samps = len(time)            # Number of system measurements [-]\n",
    "\n",
    "### Set Aside Pre-Selected Seeds for Each Inference Trial ###\n",
    "noiseSeeds = np.array([[362447,221795,415188,851692,429994,875432,34204,930263,612276,803118,\n",
    "    988942,148487,52253,217254,186481,823575,894512,936909,94514,34263\n",
    "    ,420,365133,366385,168921,262852,562762,290278,104994,687545,123494\n",
    "    ,71194,236227,654985,458480,60785,5978,164453,795134,733948,994617\n",
    "    ,251981,497532,145974,637560,367309,503274,278070,738365,146207,919550]\n",
    "    ,[375559,596157,405683,662582,615421,873481,815801,692548,678472,977676\n",
    "    ,576723,541565,814346,994860,692387,737166,904958,484792,697589,915172\n",
    "    ,814859,791522,768595,592237,923599,500042,703901,902362,827779,171795\n",
    "    ,1793,66521,518574,520292,898293,666632,654687,426340,44827,648037\n",
    "    ,828481,247297,737387,884103,638953,415614,484558,198179,454962,849656]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODELLING FUNCTIONS FOR DYNAMICAL SYSTEM ###\n",
    "def fx(x, dt, exc=None):\n",
    "    \"\"\"\n",
    "    State transition model for a SDOF oscillator with a Bouc-Wen switch \n",
    "    state, given that alpha = 0, and therefore the Bouc-Wen component is \n",
    "    switched off.\n",
    "      \n",
    "    x = 1x8 vector of states (disp [m], vel [m/sec], Bouc-Wen disp [m]) \n",
    "                and parameters to be inferred (log(xi),log(wn), log(beta), \n",
    "                log(n), log(gamma)). \n",
    "    dt = sampling rate [sec]\n",
    "    exc = input excitation at current time step [m/sec^2]\n",
    "    \"\"\"\n",
    "    if exc is None:\n",
    "        exc = np.zeros(x[1].shape)\n",
    "      \n",
    "    par = np.exp(x[3:]) \n",
    "            \n",
    "    x1dot = x[0] + dt*x[1]\n",
    "    x2dot = x[1] + dt*(-exc - (2*par[0]*par[1])*x[1] - np.square(par[1])*x[2])\n",
    "    x3dot = x[2] + dt*(x[1] - par[2]*np.absolute(x[1])*\n",
    "                np.power(np.absolute(x[2]), par[3]-1)*x[2] \n",
    "                - par[4]*x[1]*np.power(np.absolute(x[2]), par[3]))\n",
    "\n",
    "    return np.concatenate((np.stack((x1dot, x2dot, x3dot), axis=0)\n",
    "                           , x[3:]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ADVI\n",
    "The ADVI implementation demonstrated herein is an in-house code based on the work of Kucukelbir et al. (2017). The process below sets up the optimization using the functions and classes built in the files stochMod_annealing.py, which sets up a class to describe the priors and a class to describe the stochastic dynamical system model, and varFam.py, which sets up a class to describe the variational family. Detailed descriptions of these classes and their functionalities are given in the respective .py files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Constants ##\n",
    "nConst = 4\n",
    "# Measurement Noise #\n",
    "vConst = stochMod.Deterministic('constants', 0, R, name = 'Acceleration Noise')\n",
    "# Process Noise #\n",
    "w1Const = stochMod.Deterministic('constants', 1, Q[0], name = 'Process Noise on X1')\n",
    "w2Const = stochMod.Deterministic('constants', 2, Q[1], name = 'Process Noise on X2')\n",
    "w3Const = stochMod.Deterministic('constants', 3, Q[2], name = 'Process Noise on X3')\n",
    "\n",
    "## Set Prior on the Initial Conditions of the States ##\n",
    "x1Inf = stochMod.Normal('disp', 0, 0, 0.05, name = 'X1(0)')\n",
    "x2Inf = stochMod.Normal('vel', 0, 0, 0.05, name = 'X2(0)')\n",
    "x3Inf = stochMod.Normal('rdisp', 0, 0, 0.05, name = 'X3(0)')\n",
    "\n",
    "## Load Prior Distribution on the Parameters ##\n",
    "parPriors = np.loadtxt('../04-Data/parameter_priors.txt')\n",
    "\n",
    "## Define Variational Inference Parameters ##   \n",
    "numEpoch = 2                           # Number of Epochs\n",
    "epochIter = np.array([600000, 1400000])  # Iterations per Epoch\n",
    "storeInt = 100                         # Interval at which to store inference history\n",
    "optIter = int(np.sum(epochIter)/storeInt) + 1 # Total number of optimization steps\n",
    "nStore = int(epochIter[0]/storeInt)    # Number of values to store per epoch\n",
    "reportInt = epochIter[0]/10            # Interval at which to report inference history\n",
    "annealSelect = True                    # Option to use KL Annealing \n",
    "\n",
    "## Set Parameters and Data for Stochastic Gradient Descent ##\n",
    "MCSamp = 6                    # Number of MC Samples for SGD\n",
    "data = torch.tensor(respAcc)  # observed data for optimization\n",
    "vibr = torch.tensor(inpAcc)   # input acceleration\n",
    "lr = {'disp': np.array([0.0001,0.00005]),\n",
    "       'vel': np.array([0.0001,0.00005]),\n",
    "       'rdisp': np.array([0.0001,0.00005]),\n",
    "       'par': np.array([0.0001,0.00005])}  # Learning Rate for Adam SGD\n",
    "\n",
    "\n",
    "### Generate Storage Over Inferred States/Parameters ###\n",
    "muHist = np.zeros((parPriors.shape[0],nPar, optIter))\n",
    "    # mu value of the inferred parameters for each inference trial\n",
    "    # over the optimization history. This is what VI directly\n",
    "    # outputs\n",
    "stdHist = np.zeros((parPriors.shape[0],nPar, optIter))\n",
    "    # standard deviation of the inferred parameters for each \n",
    "    # inference trial over the optimization history. This is \n",
    "    # what VI directly outputs\n",
    "meanHist = np.zeros((parPriors.shape[0],nPar, optIter))\n",
    "    # mean of the underlying parameters for each inference trial over\n",
    "    # the observation period. This measure takes the mean of the \n",
    "    # lognormal posteriors of the parameters.\n",
    "modeHist = np.zeros((parPriors.shape[0],nPar, optIter))\n",
    "    # mode of the underlying parameters for each inference trial over\n",
    "    # the observation period. This measure takes the mode of the \n",
    "    # lognormal posteriors of the parameters.\n",
    "muStates = np.zeros((parPriors.shape[0],nState, samps))\n",
    "    # mu value of the final inferred states for each inference \n",
    "    # trial at the end of the optimization history. This is \n",
    "    # what VI directly outputs\n",
    "stdStates = np.zeros((parPriors.shape[0],nState, samps))\n",
    "    # standard deviation of the final inferred parameters for each \n",
    "    # inference trial at the end of the optimization history.  \n",
    "    # This is what VI directly outputs\n",
    "modStates = np.zeros((parPriors.shape[0],numInf, samps))\n",
    "    # Response history of the inferred system given the input\n",
    "    # excitation. Essentially, we're remodeling the behavior of \n",
    "    # the system given our selections on point estimates of the \n",
    "    # parameters from the posterior. \n",
    "runTime = np.zeros((parPriors.shape[0]))\n",
    "    # Computational time for each inference trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For Each Inference Trial... ##\n",
    "for j in range(parPriors.shape[0]):\n",
    "    ## Ensure Repeatability ##\n",
    "    torchSeed = noiseSeeds[0,j]\n",
    "    torch.manual_seed(torchSeed)\n",
    "    \n",
    "    npSeed = noiseSeeds[1,j]\n",
    "    np.random.seed(npSeed)\n",
    "    \n",
    "    # Set Prior on Parameters #\n",
    "    mu0 = parPriors[j,::2]\n",
    "    sig0 = parPriors[j,1::2]\n",
    "    xiInf = stochMod.LogNormal('par', 0, mu0[0], sig0[0], name='Xi')\n",
    "    wnInf = stochMod.LogNormal('par', 1, mu0[1], sig0[1], name='Wn')\n",
    "    BInf = stochMod.LogNormal('par', 2, mu0[2], sig0[2], name='Beta')\n",
    "    nInf = stochMod.LogNormal('par', 3, mu0[3], sig0[3], name='N')\n",
    "    GInf = stochMod.LogNormal('par', 4, mu0[4], sig0[4], name='Gamma')\n",
    "    \n",
    "    ## Define Stochastic Time Series Model ##\n",
    "    allPriors = {'x1': x1Inf, 'x2':x2Inf, 'x3':x3Inf,\n",
    "             'xi':xiInf, 'wn': wnInf, \n",
    "             'beta':BInf, 'n': nInf, 'gamma':GInf,\n",
    "             'w1': w1Const, 'w2': w2Const, 'w3': w3Const,\n",
    "             'v':vConst}\n",
    "    sdof = stochMod.BWSDOF_Euler(dt, allPriors)\n",
    "    \n",
    "    ## Define Variational Family ##\n",
    "    guide = {'disp': varFam.NormalTri(size=samps, storage = nStore),\n",
    "            'vel': varFam.NormalTri(size=samps, storage = nStore),\n",
    "             'rdisp': varFam.NormalTri(size=samps, storage = nStore),\n",
    "            'par': varFam.NormalDiag(size = nPar, storage = nStore, \n",
    "                                m = torch.tensor([xiInf.transMean, wnInf.transMean, \n",
    "                                                  BInf.transMean, nInf.transMean, GInf.transMean]), \n",
    "                                log_s = torch.tensor([np.log(np.sqrt(xiInf.transVar)),\n",
    "                                                      np.log(np.sqrt(wnInf.transVar)),\n",
    "                                                     np.log(np.sqrt(BInf.transVar)),\n",
    "                                                     np.log(np.sqrt(nInf.transVar)),\n",
    "                                                     np.log(np.sqrt(GInf.transVar))])),\n",
    "            'constants': varFam.Deterministic(size = nConst, storage = nStore, \n",
    "                                             m=torch.tensor(np.array([vConst.par[0], \n",
    "                                             w1Const.par[0], w2Const.par[0], w3Const.par[0]])))}\n",
    "            # This dictionary developes a parameterized distribution over the parameters\n",
    "            # to be inferred which approximates the true posterior.\n",
    "            \n",
    "    \n",
    "    ## Run ADVI ##\n",
    "    t0 = timeit.time()\n",
    "    (elboHist, infMeans, infLs) = stochMod.inferTimeSeries(sdof,guide,data,vibr, \n",
    "                        lr, MC=MCSamp,nEpoch=numEpoch,epochIter=epochIter,\n",
    "                        sInt=storeInt,rInt=reportInt, anneal = annealSelect)\n",
    "\n",
    "    tf = timeit.time()\n",
    "    runTime[j] = ((tf-t0)/60)\n",
    "    \n",
    "    ## Store Inference Results on the States ##\n",
    "    muStates[j,0,:] = infMeans['disp'][-1]\n",
    "    stdStates[j,0,:] = infLs['disp'][-1]\n",
    "    muStates[j,1,:] = infMeans['vel'][-1]\n",
    "    stdStates[j,1,:] = infLs['vel'][-1]\n",
    "    muStates[j,2,:] = infMeans['rdisp'][-1]\n",
    "    stdStates[j,2,:] = infLs['rdisp'][-1]  \n",
    "\n",
    "    ## Store Inference Results on the Parameters ##\n",
    "    parMeans = infMeans['par']\n",
    "    parStds = infLs['par']\n",
    "    muHist[j] = np.transpose(parMeans)\n",
    "    stdHist[j] = np.transpose(parStds)\n",
    "    meanHist[j] = np.transpose(np.exp(parMeans + np.square(parStds)/2.))\n",
    "    modeHist[j] = np.transpose(np.exp(parMeans - np.square(parStds)))\n",
    "\n",
    "    ### Rerun Model with Identified Parameters ###\n",
    "    modStates[j,:,0] = np.concatenate((np.zeros((nState,)), np.log(modeHist[j,:,-1])))\n",
    "    for k in range(1,samps):\n",
    "        modStates[j,:,k] = fx(modStates[j,:,k-1], dt, exc=inpAcc[k-1]) \n",
    "   \n",
    "    ## Print Results Summary ##\n",
    "    print('\\nIteration %d' %(j))\n",
    "    print('\\tComputation Time = %d minutes and %d seconds' %(np.floor((tf-t0)/60), \n",
    "                                                        np.floor((tf-t0) - 60*np.floor((tf-t0)/60))))\n",
    "    print('\\tMode of Final Parameter Distributions: \\n\\t\\txi = %.4f,\\n\\t\\twn = %.4f,\\n\\t\\tbeta = %.4f,\\n\\t\\tn = %.4f,\\n\\t\\tgamma = %.4f\\n'\n",
    "          %(modeHist[j,0,-1],modeHist[j,1,-1],modeHist[j,2,-1],modeHist[j,3,-1],modeHist[j,4,-1]))\n",
    "\n",
    "np.savez(outFile, muHist = muHist,stdHist=stdHist, \n",
    "         meanHist=meanHist, modeHist=modeHist, modStates=modStates, \n",
    "         muStates = muStates, stdStates = stdStates, runTime = runTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Inference Output\n",
    "As you may have noticed, the outputs for the variational inference case are structured slightly differently than the outputs for the remainder of the cases. That is because the variational inference case given herein is run in batch mode, whereas the remainder of the example algorithms run in a filtering mode. Because of this, the generalized plotting code, '05-Plot_Results', will not be able to handle the results from the VI case automatically. Instead, I've plotted them here.\n",
    "\n",
    "### Load Inference Data\n",
    "This becomes an optional start point in the code. If the data for the UKF has already been generated, it can simply be loaded in for the predictive analysis instead of rerunning the previous block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outData = np.load(outFile + '.npz')\n",
    "\n",
    "muHist = outData['muHist']         # optimization history of untransformed par means\n",
    "stdHist = outData['stdHist']       # optimization history of par standard deviations\n",
    "meanHist = outData['meanHist']     # optimization history of transformed par means\n",
    "modeHist = outData['modeHist']     # optimization history of transformed par modes\n",
    "muStates = outData['muStates']       # final optimized value on the untransformed state means\n",
    "stdStates = outData['stdStates']     # final optimized value on the untransformed state standard deviations\n",
    "modStates = outData['modStates']     # states that have been remodeled based on the final modes of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comprehensive Results\n",
    "This set of figures is intended to give an overview of the results for all inference trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Parameters ##\n",
    "fig, ax = plt.subplots(1,2,figsize = (16,4))\n",
    "ax[0].hist(modeHist[:,0,-1], bins = np.concatenate((np.arange(0,0.3, 0.01), np.array([1]))), label = r\"\\xi\", alpha = 0.5)\n",
    "ax[0].set_title(r'$\\xi$ Identification')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_xlabel(r'$\\xi$ Value [-]')\n",
    "ax[0].set_xlim((0,0.3))\n",
    "ax[0].grid(1)\n",
    "#ax[0].legend()\n",
    "\n",
    "ax[1].hist(modeHist[:,1,-1], bins = np.concatenate((np.arange(0,5, 0.1), np.array([100]))), label = r'$\\omega_n$', alpha = 0.5)\n",
    "ax[1].set_title(r'$\\omega_n$ Identification')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_xlabel(r'$\\omega_n$ Value [rad/s]')\n",
    "ax[1].set_xlim((0,5))\n",
    "ax[1].grid(1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "## Nonlinear Parameters ##\n",
    "fig, ax = plt.subplots(1,3,figsize = (16,4))\n",
    "ax[0].hist(modeHist[:,2,-1], bins = np.concatenate((np.arange(0,25, 0.5), np.array([1000]))), label = r'$\\beta$', alpha = 0.5)\n",
    "ax[0].set_title(r'$\\beta$ Identification')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_xlabel(r'$\\beta$ Value [1/$m^N$]')\n",
    "ax[0].set_xlim((0,25))\n",
    "ax[0].grid(1)\n",
    "#ax[0].legend()\n",
    "\n",
    "ax[1].hist(modeHist[:,3,-1], bins = np.concatenate((np.arange(0,10, 0.25), np.array([30]))), label = \"N\", alpha = 0.5)\n",
    "ax[1].set_title('N Identification')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_xlabel('N Value [-]')\n",
    "ax[1].set_xlim((0,10))\n",
    "ax[1].grid(1)\n",
    "\n",
    "ax[2].hist(modeHist[:,4,-1], bins = np.concatenate((np.arange(0,25, 0.5), np.array([1000]))), label = r'$\\gamma$', alpha = 0.5)\n",
    "ax[2].set_title(r'$\\gamma$ Identification')\n",
    "ax[2].set_ylabel('Count')\n",
    "ax[2].set_xlabel(r'$\\gamma$ Value [1/$m^N$]')\n",
    "ax[2].set_xlim((0,25))\n",
    "ax[2].grid(1)\n",
    "\n",
    "plt.tight_layout()   \n",
    "\n",
    "\n",
    "## State Error ##\n",
    "RMSDisp = np.sqrt(np.mean(np.square(states[0] - modStates[:,0,:]), axis=1))\n",
    "findNans = np.isnan(RMSDisp)\n",
    "RMSDisp[findNans] = 5\n",
    "RMSVel = np.sqrt(np.mean(np.square(states[1] - modStates[:,1,:]), axis=1))\n",
    "findNans = np.isnan(RMSVel)\n",
    "RMSVel[findNans] = 5\n",
    "RMSRDisp = np.sqrt(np.mean(np.square(states[2] - modStates[:,2,:]), axis=1))\n",
    "findNans = np.isnan(RMSRDisp)\n",
    "RMSRDisp[findNans] = 20\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize = (16,4))\n",
    "ax[0].hist(RMSDisp, bins = np.concatenate((np.arange(0,2, 0.1), np.array([20]))), label = r'Disp.', alpha = 0.5)\n",
    "ax[0].set_title(r'Disp. Error')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_xlabel(r'Error Value [--]')\n",
    "ax[0].set_xlim((0,2))\n",
    "ax[0].grid(1)\n",
    "#ax[0].legend()\n",
    "\n",
    "ax[1].hist(RMSVel, bins = np.concatenate((np.arange(0,2, 0.1), np.array([20]))), label = \"Vel.\", alpha = 0.5)\n",
    "ax[1].set_title('Vel. Error')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_xlabel('Error Value [--]')\n",
    "ax[1].set_xlim((0,2))\n",
    "ax[1].grid(1)\n",
    "\n",
    "ax[2].hist(RMSRDisp, bins = np.concatenate((np.arange(0,20, 0.4), np.array([30]))), label = r'BW Disp.', alpha = 0.5)\n",
    "ax[2].set_title('BW Disp. Error')\n",
    "ax[2].set_ylabel('Count')\n",
    "ax[2].set_xlabel('Error Value [m]')\n",
    "ax[2].set_xlim((0,20))\n",
    "ax[2].grid(1)\n",
    "\n",
    "plt.tight_layout()  \n",
    "\n",
    "## State Error ##\n",
    "index = np.argmin(RMSDisp)\n",
    "\n",
    "figure, ax=plt.subplots(2,2,figsize=(16,8)) \n",
    "ax[0,0].plot(time, states[0], '-', label='True State')\n",
    "ax[0,0].plot(time, modStates[index, 0,:], '--', label='Best Re-modeled State')\n",
    "ax[0,0].set_xlabel('Time [sec]')\n",
    "ax[0,0].set_ylabel('Disp. [m]')\n",
    "ax[0,0].set_title('Displacement Inference')\n",
    "ax[0,0].set_xlim((0, time[-1]))\n",
    "ax[0,0].grid(1)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].plot(time, states[1], '-', label='True State')\n",
    "ax[0,1].plot(time, modStates[index, 1,:], '--', label='Best Re-modeled State')\n",
    "ax[0,1].set_xlabel('Time [sec]')\n",
    "ax[0,1].set_ylabel('Vel. [m/sec]')\n",
    "ax[0,1].set_title('Velocity Inference')\n",
    "ax[0,1].set_xlim((0, time[-1]))\n",
    "ax[0,1].grid(1)\n",
    "\n",
    "ax[1,0].plot(time, states[2], '-', label='True State')\n",
    "ax[1,0].plot(time, modStates[index, 2,:], '--', label='Best Re-modeled State')\n",
    "ax[1,0].set_xlabel('Time [sec]')\n",
    "ax[1,0].set_ylabel('BW Disp. [m]')\n",
    "ax[1,0].set_title('BW Disp. Inference')\n",
    "ax[1,0].set_xlim((0, time[-1]))\n",
    "ax[1,0].grid(1)\n",
    "\n",
    "ax[1,1].plot(states[0,:],states[2,:], label=\"True State\")\n",
    "ax[1,1].plot(modStates[index,0,:],modStates[index,2,:], label=\"Best Re-modeled State\")\n",
    "ax[1,1].set_xlabel('Disp. [m]')\n",
    "ax[1,1].set_ylabel('Bouc-Wen Disp. [m]')\n",
    "ax[1,1].set_title('Bouc-Wen Hysteresis')\n",
    "ax[1,1].set_xlim((-1,1))\n",
    "ax[1,1].set_ylim((-1,1))\n",
    "ax[1,1].grid(1)\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results for One Case\n",
    "This set of figures singles out a specific inference case and examines the results in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Results ###\n",
    "caseSelect = index\n",
    "\n",
    "## States ##\n",
    "figure, ax=plt.subplots(3,1,figsize=(16,12)) \n",
    "ax[0].plot(time, states[0], '-', label='True State')\n",
    "ax[0].plot(time, muStates[caseSelect, 0], ':', label='Inferred State')\n",
    "ax[0].plot(time, modStates[caseSelect, 0], '--', label='Re-modeled State')\n",
    "ax[0].set_xlabel('Time [sec]')\n",
    "ax[0].set_ylabel('Disp. [m]')\n",
    "ax[0].set_title('Displacement Inference')\n",
    "ax[0].set_xlim((0, time[-1]))\n",
    "ax[0].grid(1)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(time, states[1], '-', label='True State')\n",
    "ax[1].plot(time, muStates[caseSelect, 1], ':', label='Inferred State')\n",
    "ax[1].plot(time, modStates[caseSelect, 1], '--', label='Re-modeled State')\n",
    "ax[1].set_xlabel('Time [sec]')\n",
    "ax[1].set_ylabel('Vel. [m/sec]')\n",
    "ax[1].set_title('Velocity Inference')\n",
    "ax[1].set_xlim((0, time[-1]))\n",
    "ax[1].grid(1)\n",
    "\n",
    "ax[2].plot(time, states[2], '-', label='True State')\n",
    "ax[2].plot(time, muStates[caseSelect, 2], ':', label='Inferred State')\n",
    "ax[2].plot(time, modStates[caseSelect, 2], '--', label='Re-modeled State')\n",
    "ax[2].set_xlabel('Time [sec]')\n",
    "ax[2].set_ylabel('BW Disp. [m]')\n",
    "ax[2].set_title('BW Disp. Inference')\n",
    "ax[2].set_xlim((0, time[-1]))\n",
    "ax[2].grid(1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "## Parameters ##\n",
    "optRange = np.arange(0,np.sum(epochIter)+1,storeInt)\n",
    "figure, ax=plt.subplots(1,2,figsize=(16,4)) \n",
    "ax[0].plot(optRange, par[0]*np.ones(len(optRange)), '-', label='True Parameter')\n",
    "ax[0].plot(optRange, modeHist[caseSelect, 0], ':', label='Inferred Parameter')\n",
    "ax[0].set_xlabel(r'Optimization Iterations $[x10^5]$')\n",
    "ax[0].set_ylabel(r'$\\xi$ [--]')\n",
    "ax[0].set_title(r'$\\xi$ Inference')\n",
    "ax[0].set_xlim((0, optRange[-1]))\n",
    "ax[0].grid(1)\n",
    "ax[0].legend()     \n",
    "\n",
    "ax[1].plot(optRange, par[1]*np.ones(len(optRange)), '-', label='True Parameter')\n",
    "ax[1].plot(optRange, modeHist[caseSelect, 1], ':', label='Inferred Parameter')\n",
    "ax[1].set_xlabel(r'Optimization Iterations $[x10^5]$')\n",
    "ax[1].set_ylabel(r'$\\omega_n$ [--]')\n",
    "ax[1].set_title(r'$\\omega_n$ Inference')\n",
    "ax[1].set_xlim((0, optRange[-1]))\n",
    "ax[1].grid(1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figure, ax=plt.subplots(1,3,figsize=(16,4)) \n",
    "ax[0].plot(optRange, 2*np.ones(len(optRange)), '-', label='True Parameter')\n",
    "ax[0].plot(optRange, modeHist[caseSelect, 2], ':', label='Inferred Parameter')\n",
    "ax[0].set_xlabel(r'Optimization Iterations $[x10^5]$')\n",
    "ax[0].set_ylabel(r'$\\beta$ [--]')\n",
    "ax[0].set_title(r'$\\beta$ Inference')\n",
    "ax[0].set_xlim((0, optRange[-1]))\n",
    "ax[0].set_ylim((0, 20))\n",
    "ax[0].grid(1)\n",
    "ax[0].legend() \n",
    "\n",
    "ax[1].plot(optRange, 2*np.ones(len(optRange)), '-', label='True Parameter')\n",
    "ax[1].plot(optRange, modeHist[caseSelect, 3], ':', label='Inferred Parameter')\n",
    "ax[1].set_xlabel(r'Optimization Iterations $[x10^5]$')\n",
    "ax[1].set_ylabel(r'$n$ [--]')\n",
    "ax[1].set_title(r'$n$ Inference')\n",
    "ax[1].set_xlim((0, optRange[-1]))\n",
    "ax[1].grid(1)\n",
    "\n",
    "ax[2].plot(optRange, 1*np.ones(len(optRange)), '-', label='True Parameter')\n",
    "ax[2].plot(optRange, modeHist[caseSelect, 4], ':', label='Inferred Parameter')\n",
    "ax[2].set_xlabel(r'Optimization Iterations $[x10^5]$')\n",
    "ax[2].set_ylabel(r'$\\gamma$ [--]')\n",
    "ax[2].set_title(r'$\\gamma$ Inference')\n",
    "ax[2].set_xlim((0, optRange[-1]))\n",
    "ax[2].set_ylim((0, 20))\n",
    "ax[2].grid(1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Capacity of the Inferred Models\n",
    "The goal of this section is to develop a prediction of the response behavior of the system to a secondary event, given the models which have been inferred from the primary excitation. \n",
    "\n",
    "### Load Inference Data\n",
    "This becomes an optional start point in the code. If the data for the UKF has already been generated, it can simply be loaded in for the predictive analysis instead of rerunning the previous block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outData = np.load(outFile + '.npz')\n",
    "\n",
    "muHist = outData['muHist']         # optimization history of untransformed par means\n",
    "stdHist = outData['stdHist']       # optimization history of par standard deviations\n",
    "meanHist = outData['meanHist']     # optimization history of transformed par means\n",
    "modeHist = outData['modeHist']     # optimization history of transformed par modes\n",
    "muStates = outData['muStates']       # final optimized value on the untransformed state means\n",
    "stdStates = outData['stdStates']     # final optimized value on the untransformed state standard deviations\n",
    "modStates = outData['modStates']   # states that have been remodeled based on the final modes of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Secondary Input Excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predInFile = '../04-Data/Bouc-Wen/predInp_BLWN'\n",
    "predOutFile = '../04-Data/Bouc-Wen/predOutVI'\n",
    "\n",
    "infData = np.load(predInFile + '.npz')\n",
    "\n",
    "dt = infData['dt']                            # time step [sec]\n",
    "time = infData['time']                        # time history [sec]\n",
    "predBase = infData['predInp']                 # observations on input acceleration [m/sec^2]\n",
    "predStatesTrue = infData['predStatesPNoise']  # states (for post-prediction validation) [m,m/sec]\n",
    "predRespTrue = infData['predAccPMNoise']      # observations on response acceleration [m/sec^2]\n",
    "Q = infData['Qfactor']                        # process noise contributions, independent std. dev. per state [m,m/sec]\n",
    "R = infData['Rfactor']                        # measurement noise contribution [m/sec^2]\n",
    "m = infData['m']                              # mass [kg]\n",
    "ics = infData['ics']                          # true initial conditions of the system [m, m/sec]\n",
    "par = infData['par']                          # true parameters of the system [xi (-), wn (rad/sec)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictive Distribution on the States over Secondary Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Constants for Predictive Sampling ###\n",
    "nPriors = muHist.shape[0]    # Number of inference trials [-]\n",
    "nSamps = 500                 # Number of samples on the inference posterior [-]\n",
    "seeds = [8192,3245]          # seeds for random number generator\n",
    "\n",
    "### Generate Storage Over Predicted States ###\n",
    "totalSamps = np.zeros((nSamps*nPriors, nState, samps))\n",
    "    # Predicted states based on simulations results for all posterior\n",
    "    # samples from all inference trials\n",
    "meanPred = np.zeros((nPriors, nState, samps))\n",
    "    # Mean of the predicted states for each inference trial.\n",
    "stdPred = np.zeros((nPriors,nState, samps))\n",
    "    # Standard deviation of the predicted states for each inference trial. \n",
    "\n",
    "### Run Predictive Trials on All Candidate Models ###\n",
    "print('Predictive Distribution from Inferred Results')\n",
    "for j in range(nPriors):\n",
    "    print('Case %d'%(j))\n",
    "    ## Random Samples on the States and Parameters, based on Inferred Posterior ##\n",
    "    np.random.seed(seeds[0]+j)\n",
    "    rSamp = np.random.multivariate_normal(np.zeros(numInf), np.eye(numInf), nSamps)\n",
    "    muPredInp = np.concatenate((muStates[j,:,-1], muHist[j,:,-1]), axis=0)\n",
    "    stdPredInp = np.concatenate((stdStates[j,:,-1], stdHist[j,:,-1]), axis=0)\n",
    "    predSamps = muPredInp + stdPredInp*rSamp\n",
    "\n",
    "    ## Random Samples on the Transition Noise ##\n",
    "    np.random.seed(seeds[1]+j)\n",
    "    noise = Q.reshape(-1,1)*np.random.multivariate_normal(np.zeros(nState), np.eye(nState), \n",
    "                                                          (nSamps, len(time))).transpose((0, 2, 1))\n",
    "\n",
    "    ## Prepare Response Storage ##\n",
    "    predStates = np.zeros((nSamps, nState,len(time)))\n",
    "    predStates[:,:,0] = predSamps[:,:nState]\n",
    "\n",
    "    for i in range(nSamps):\n",
    "        for tt in range(1,len(time)):\n",
    "            predStates[i,:,tt] = fx(np.concatenate((predStates[i,:, tt-1], predSamps[i,nState:])), \n",
    "                                                 dt, exc = predBase[tt-1])[:nState] + noise[i,:,tt-1]\n",
    "    \n",
    "    ## Store Results from Predictive Sample Runs ##\n",
    "    meanPred[j,:,:] = np.mean(predStates, axis = 0)\n",
    "    stdPred[j,:,:] = np.sqrt(np.mean(np.square(predStates), axis=0) - np.square(meanPred[j,:,:])) \n",
    "    totalSamps[j*nSamps:(j+1)*nSamps,:,:] = predStates\n",
    "    \n",
    "### Remove Unstable Results from the Overall Assessment ###\n",
    "# Candidate models can become unstable during inference (due to \n",
    "# computational issues such as singularities in the covariance \n",
    "# matrices) or manifest instability during predictive modeling\n",
    "# due to combinations of the selected parameters which result in\n",
    "# model divergence. Here we extract these cases so that they don't \n",
    "# interfere with the statistics of the main results. \n",
    "stabilityInd = np.ones(nPriors)\n",
    "totalStabilityInd = np.ones(nPriors*nSamps)\n",
    "\n",
    "print('\\nIndices of Unstable Predictive Distributions:')\n",
    "for i in range(nPriors):\n",
    "    if (np.isnan(meanPred[i,0,-1])) or (np.absolute(meanPred[i,0,-1])>100) or (muHist[i,0,-1] == 0):\n",
    "        stabilityInd[i] = 0 \n",
    "        totalStabilityInd[i*nSamps:(i+1)*nSamps] = np.zeros(nSamps)\n",
    "        print(i)\n",
    "\n",
    "stableMeans = meanPred[stabilityInd != 0,:,:]\n",
    "stableStds = stdPred[stabilityInd != 0,:,:]\n",
    "stableSamps = totalSamps[totalStabilityInd != 0,:,:]\n",
    "\n",
    "### Statistics on all Stable Cases ###\n",
    "meanAll = np.mean(stableSamps, axis = 0)\n",
    "stdAll = np.sqrt(np.mean(np.square(stableSamps), axis=0) - np.square(meanAll)) \n",
    "\n",
    "### Save Output ###\n",
    "np.savez(predOutFile, meanPred = meanPred,stdPred=stdPred, \n",
    "         stableMeans=stableMeans, stableStds=stableStds, meanAll=meanAll, stdAll=stdAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictive Output\n",
    "Unlike the inference results, the predictive results take the same form between all inference algorithms. Therefore, to see the predictive results, please navigate to the general plotting code, '05-Plot Results'. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
